{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN HolyGrail.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wciTnnifNtU0"
      },
      "outputs": [],
      "source": [
        "#First step :- Load libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense      #neural net layers\n",
        "import pandas as pd                  \n",
        "from sklearn.model_selection import train_test_split   #to spit dataset into train,validation,test \n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install -q xlrd           #library to read from excel sheet\n",
        "import io                      #input-output = io\n",
        "from google.colab import files #important to locally save files\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth    #to use Google Cloud SDK\n",
        "from pydrive.drive import GoogleDrive  \n",
        "from google.colab import auth           #to authenticate gmail ID\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()   #to authenticate gmail ID in Google Cloud SDK\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "import math\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import cm\n",
        "from matplotlib import gridspec\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "# to find MSE of overall data\n",
        "def mse(actual, pred): \n",
        "    actual, pred = np.array(actual), np.array(pred)\n",
        "    return np.square(np.subtract(actual,pred)).mean()\n",
        "\n",
        "\n",
        "def rmse(actual, pred): \n",
        "    actual, pred = np.array(actual), np.array(pred)\n",
        "    return np.sqrt(np.square(np.subtract(actual,pred)).mean())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Store the data set\n",
        "file_id = '1oQw9vdiTsmsvOayjpskpVj_ZvBLPCV' #file key of google sheet\n",
        "\n",
        "downloaded = drive.CreateFile({'id': file_id}) \n",
        "downloaded.GetContentFile('data.xlsx')   \n",
        "\n",
        "df = pd.read_excel('data.xlsx')\n",
        "\n",
        "dataset = df.values                        #Convert the data into an array\n",
        "inputs  = dataset[1:212,0:4]               #set input data \n",
        "targets = dataset[1:212,4:5]               #set output data"
      ],
      "metadata": {
        "id": "T-XQ370UN5jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#How to split this into train & test data?\n",
        "X_train, x_test, Y_train, y_test = train_test_split(inputs, targets, test_size=0.2, random_state = 4)   \n",
        "#Split train into train & validation(80% train, 20% test, 20% val)\n",
        "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=1) \n",
        "#Why 0.25? Because 0.25 x 0.8 = 0.2 \n",
        "\n",
        "#from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(x_train)   #Standardizing all the values\n",
        "X_test = sc.fit_transform(x_test)\n",
        "Y_train = sc.fit_transform(y_train)\n",
        "Y_test = sc.fit_transform(y_test)\n",
        "INPUTS = sc.fit_transform(inputs)\n",
        "TARGETS = sc.fit_transform(targets)\n",
        "X_val = sc.fit_transform(x_val)\n",
        "Y_val = sc.fit_transform(y_val)"
      ],
      "metadata": {
        "id": "H0sNbaXFN9C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session() #close the previous model variable before starting a fresh run"
      ],
      "metadata": {
        "id": "2jTbf-ciRcny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Voila, now lets build Neural network model\n",
        "#from keras.layers import LeakyReLU\n",
        "model = Sequential([                                                      #Sequential specifies to keras that we are creating model sequentially and the output of each layer we add is input to the next layer we specify.\n",
        "                    Dense(16, activation = 'sigmoid', input_shape=(4, )), #you only need to specify no. of attibutes aka input_shape for first HIDDEN layer\n",
        "                    Dense(48, activation = 'relu'),                       #second HIDDEN layer\n",
        "                    Dense(16, activation = 'tanh') \n",
        "])\n",
        "model.add(Dense(1,))                                 #another way to add a layer. This is the output layer. \n"
      ],
      "metadata": {
        "id": "WdySwamxOGzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To specify the loss function and the optimizer\n",
        "from keras import optimizers\n",
        "from keras.optimizers import adam_v2, adagrad_v2, adamax_v2\n",
        "import random\n",
        "#opt = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
        "model.compile(loss='mse', optimizer = adam_v2.Adam(learning_rate=0.01), metrics=['mae'])\n",
        "\n",
        "#Training the model\n",
        "hist = model.fit(X_train, Y_train, validation_split = 0,validation_data = (X_val,Y_val), epochs= 80, batch_size=2)   "
      ],
      "metadata": {
        "id": "UfSYS_ALOKB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To extract weights from the Neural network\n",
        "print(model.get_weights())"
      ],
      "metadata": {
        "id": "FDLd91JtOQD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize the training loss and the validation loss to see if the model is overfitting\n",
        "plt.plot(hist.history['loss'], 'k-')\n",
        "plt.plot(hist.history['val_loss'], 'b--')\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val', 'test'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "#Visualize the training accuracy and the validation accuracy to see if the model is overfitting\n",
        "plt.plot(hist.history['mae'], 'k-')\n",
        "plt.plot(hist.history['val_mae'], 'b--')\n",
        "plt.title('', fontsize = 16)\n",
        "plt.ylabel(' mae')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fd5N_TBfOWV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inverse transform\n",
        "y_pred = model.predict(INPUTS)\n",
        "y_pred_inv = sc.inverse_transform(y_pred)\n",
        "\n",
        "y_predtrain = model.predict(X_train)\n",
        "y_predtrain_inv = sc.inverse_transform(y_predtrain)\n",
        "\n",
        "y_predval = model.predict(X_val)\n",
        "y_predval_inv = sc.inverse_transform(y_predval)\n",
        "\n",
        "y_predtest = model.predict(X_test)\n",
        "y_predtest_inv = sc.inverse_transform(y_predtest)"
      ],
      "metadata": {
        "id": "W7mOh0e3OX5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mse(targets, y_pred_inv))\n",
        "print(rmse(targets, y_pred_inv))"
      ],
      "metadata": {
        "id": "I8we7x46Oddp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using sklearn\n",
        "from sklearn.metrics import r2_score\n",
        "r2_all = r2_score(targets, y_pred_inv)\n",
        "r2_test = r2_score(y_test, y_predtest_inv)\n",
        "r2_train = r2_score(y_train, y_predtrain_inv)\n",
        "r2_val = r2_score(y_val, y_predval_inv)\n",
        "print ('r2 Train',r2_train)\n",
        "print ('r2 Test',r2_test)\n",
        "print ('r2 validation',r2_val)\n",
        "print ('r2 overall',r2_all)"
      ],
      "metadata": {
        "id": "5EUWfpGSOnL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating R2 values of standardized variables. The results are slightly better\n",
        "from sklearn.metrics import r2_score\n",
        "r2_all1 = r2_score(TARGETS, y_pred)\n",
        "r2_test1 = r2_score(Y_test, y_predtest)\n",
        "r2_train1 = r2_score(Y_train, y_predtrain)\n",
        "r2_val1 = r2_score(Y_val, y_predval)\n",
        "print ('r2 Train',r2_train1)\n",
        "print ('r2 Test',r2_test1)\n",
        "print ('r2 validation',r2_val1)\n",
        "print ('r2 overall',r2_all1)"
      ],
      "metadata": {
        "id": "aj8ij6fGSCvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting overall fit\n",
        "z=[]\n",
        "a=[]\n",
        "e=[]\n",
        "\n",
        "for i in range(0,len(y_pred)):\n",
        "  z.append(inputs[i][1])\n",
        "  a.append(targets[i][0])\n",
        "  e.append(y_pred_inv[i][0])\n",
        "r=np.array(z)\n",
        "n=np.array(a)\n",
        "t = np.array(e)\n",
        "\n",
        "plt.scatter(a,e)\n",
        "plt.xlabel('Actual values')\n",
        "plt.ylabel('Predicted values')\n",
        "\n",
        "plt.plot(np.unique(a), np.poly1d(np.polyfit(a, e, 1))(np.unique(a)))\n",
        "\n",
        "plt.text(0.6, 0.5, 'R-squared = %0.2f' % r2_all)"
      ],
      "metadata": {
        "id": "SF_1k9DBOsY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting training fit\n",
        "z=[]\n",
        "a=[]\n",
        "e=[]\n",
        "\n",
        "for i in range(0,len(Y_train)):\n",
        "  z.append(x_train[i][1])\n",
        "  a.append(y_train[i][0])\n",
        "  e.append(y_predtrain_inv[i][0])\n",
        "r=np.array(z)\n",
        "n=np.array(a)\n",
        "t = np.array(e)\n",
        "\n",
        "plt.scatter(a,e)\n",
        "plt.xlabel('Actual values')\n",
        "plt.ylabel('Predicted values')\n",
        "\n",
        "plt.plot(np.unique(a), np.poly1d(np.polyfit(a, e, 1))(np.unique(a)))\n",
        "\n",
        "plt.text(0.6, 0.5, 'R-squared = %0.2f' % r2_train)"
      ],
      "metadata": {
        "id": "Gwcq0JnTOxlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting testing fit\n",
        "z=[]\n",
        "a=[]\n",
        "e=[]\n",
        "\n",
        "for i in range(0,len(Y_test)):\n",
        "  z.append(x_test[i][1])\n",
        "  a.append(y_test[i][0])\n",
        "  e.append(y_predtest_inv[i][0])\n",
        "r=np.array(z)\n",
        "n=np.array(a)\n",
        "t = np.array(e)\n",
        "\n",
        "plt.scatter(a,e)\n",
        "plt.xlabel('Actual values')\n",
        "plt.ylabel('Predicted values')\n",
        "\n",
        "plt.plot(np.unique(a), np.poly1d(np.polyfit(a, e, 1))(np.unique(a)))\n",
        "plt.text(0.6, 0.5, 'R-squared = %0.2f' % r2_test)\n"
      ],
      "metadata": {
        "id": "ZUfKlrbzOz_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting validation fit\n",
        "z=[]\n",
        "a=[]\n",
        "e=[]\n",
        "\n",
        "for i in range(0,len(Y_val)):\n",
        "  z.append(x_val[i][1])\n",
        "  a.append(y_val[i][0])\n",
        "  e.append(y_predval_inv[i][0])\n",
        "r=np.array(z)\n",
        "n=np.array(a)\n",
        "t = np.array(e)\n",
        "\n",
        "plt.scatter(a,e)\n",
        "plt.xlabel('Actual values')\n",
        "plt.ylabel('Predicted values')\n",
        "\n",
        "plt.plot(np.unique(a), np.poly1d(np.polyfit(a, e, 1))(np.unique(a)))\n",
        "plt.text(0.6, 0.5, 'R-squared = %0.2f' % r2_val)\n"
      ],
      "metadata": {
        "id": "gtQb3BDUO19w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
